#!/usr/bin/env python3
"""
Cognitive LLM Integration - Neural Language Augmentation
========================================================

This module integrates Large Language Models (LLMs) into the cognitive
synergy framework, enabling natural language understanding, hypothesis
generation, and neural-symbolic reasoning.

Key Features:
- LLM-guided pattern discovery
- Natural language hypothesis generation
- Semantic enrichment of hypergraph memory
- Neural-symbolic bridge through language

Author: OpenCog Collection Contributors
License: GPL-3.0+
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from openai import OpenAI

logger = logging.getLogger(__name__)


@dataclass
class CognitiveHypothesis:
    """Represents a hypothesis generated by LLM-augmented cognition."""
    hypothesis_text: str
    confidence: float
    supporting_atoms: List[str]
    generated_by: str
    reasoning_trace: str


class LLMCognitiveAugmenter:
    """
    Augments cognitive synergy with LLM capabilities for natural language
    understanding and hypothesis generation.
    """
    
    def __init__(self, model: str = "gpt-4.1-mini"):
        """
        Initialize the LLM cognitive augmenter.
        
        Args:
            model: Model to use (gpt-4.1-mini, gpt-4.1-nano, gemini-2.5-flash)
        """
        self.client = OpenAI()  # API key pre-configured in environment
        self.model = model
        self.conversation_history = []
        
        logger.info(f"Initialized LLM Cognitive Augmenter with model: {model}")
    
    def generate_hypothesis(self, context: Dict[str, Any], 
                           problem_description: str) -> CognitiveHypothesis:
        """
        Generate a hypothesis about a cognitive problem using LLM reasoning.
        
        Args:
            context: Current cognitive context (atoms, patterns, etc.)
            problem_description: Description of the problem to solve
            
        Returns:
            Generated hypothesis with confidence and reasoning
        """
        # Prepare context for LLM
        context_summary = self._summarize_context(context)
        
        prompt = f"""You are an AI cognitive scientist analyzing a cognitive architecture.

Context:
{context_summary}

Problem:
{problem_description}

Task: Generate a hypothesis about how to solve this problem using the available cognitive components. Consider:
1. Which cognitive processes should collaborate
2. What patterns might be relevant
3. How attention should be allocated
4. What synergies might emerge

Provide your hypothesis in JSON format:
{{
    "hypothesis": "your hypothesis text",
    "confidence": 0.0-1.0,
    "supporting_atoms": ["atom1", "atom2"],
    "reasoning": "step-by-step reasoning"
}}"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert in cognitive architectures and AGI."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            result_text = response.choices[0].message.content
            
            # Parse JSON response
            try:
                result = json.loads(result_text)
                hypothesis = CognitiveHypothesis(
                    hypothesis_text=result.get("hypothesis", ""),
                    confidence=result.get("confidence", 0.5),
                    supporting_atoms=result.get("supporting_atoms", []),
                    generated_by=f"LLM:{self.model}",
                    reasoning_trace=result.get("reasoning", "")
                )
                return hypothesis
            except json.JSONDecodeError:
                # Fallback if LLM doesn't return valid JSON
                hypothesis = CognitiveHypothesis(
                    hypothesis_text=result_text,
                    confidence=0.5,
                    supporting_atoms=[],
                    generated_by=f"LLM:{self.model}",
                    reasoning_trace="Direct LLM response"
                )
                return hypothesis
                
        except Exception as e:
            logger.error(f"Error generating hypothesis: {e}")
            return CognitiveHypothesis(
                hypothesis_text="Unable to generate hypothesis",
                confidence=0.0,
                supporting_atoms=[],
                generated_by="error",
                reasoning_trace=str(e)
            )
    
    def enrich_atom_semantics(self, atom_type: str, atom_name: str) -> Dict[str, Any]:
        """
        Use LLM to enrich an atom with semantic information.
        
        Args:
            atom_type: Type of the atom
            atom_name: Name of the atom
            
        Returns:
            Dictionary of semantic enrichments
        """
        prompt = f"""Analyze this cognitive atom and provide semantic enrichment:

Atom Type: {atom_type}
Atom Name: {atom_name}

Provide semantic information in JSON format:
{{
    "description": "brief description",
    "related_concepts": ["concept1", "concept2"],
    "cognitive_role": "role in cognitive processing",
    "attention_priority": 0.0-1.0
}}"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a semantic analysis expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=200
            )
            
            result_text = response.choices[0].message.content
            enrichment = json.loads(result_text)
            return enrichment
            
        except Exception as e:
            logger.error(f"Error enriching atom semantics: {e}")
            return {
                "description": atom_name,
                "related_concepts": [],
                "cognitive_role": "unknown",
                "attention_priority": 0.5
            }
    
    def discover_patterns_with_llm(self, atoms: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Use LLM to discover high-level patterns in cognitive data.
        
        Args:
            atoms: List of atoms to analyze
            
        Returns:
            List of discovered patterns
        """
        # Limit atoms to avoid token limits
        sample_atoms = atoms[:20] if len(atoms) > 20 else atoms
        
        atoms_summary = "\n".join([
            f"- {atom.get('atom_type', 'Unknown')}: {atom.get('name', 'unnamed')}"
            for atom in sample_atoms
        ])
        
        prompt = f"""Analyze these cognitive atoms and identify high-level patterns:

Atoms:
{atoms_summary}

Identify patterns such as:
1. Conceptual clusters
2. Causal relationships
3. Hierarchical structures
4. Functional groupings

Provide patterns in JSON format:
{{
    "patterns": [
        {{
            "type": "pattern type",
            "description": "pattern description",
            "atoms_involved": ["atom1", "atom2"],
            "confidence": 0.0-1.0
        }}
    ]
}}"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a pattern recognition expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                max_tokens=400
            )
            
            result_text = response.choices[0].message.content
            result = json.loads(result_text)
            return result.get("patterns", [])
            
        except Exception as e:
            logger.error(f"Error discovering patterns: {e}")
            return []
    
    def suggest_synergy_opportunities(self, 
                                     processes: List[Dict[str, Any]]) -> List[str]:
        """
        Use LLM to suggest cognitive synergy opportunities.
        
        Args:
            processes: List of active cognitive processes
            
        Returns:
            List of synergy suggestions
        """
        processes_summary = "\n".join([
            f"- {p.get('process_id', 'unknown')}: {p.get('process_type', 'unknown')} "
            f"(priority: {p.get('priority', 0.5)})"
            for p in processes
        ])
        
        prompt = f"""Analyze these cognitive processes and suggest synergy opportunities:

Active Processes:
{processes_summary}

Suggest ways these processes could collaborate for cognitive synergy. Consider:
1. Complementary capabilities
2. Information sharing opportunities
3. Bottleneck resolution strategies
4. Emergent capabilities

Provide suggestions as a JSON array of strings."""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a cognitive synergy expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=300
            )
            
            result_text = response.choices[0].message.content
            suggestions = json.loads(result_text)
            
            if isinstance(suggestions, list):
                return suggestions
            elif isinstance(suggestions, dict) and "suggestions" in suggestions:
                return suggestions["suggestions"]
            else:
                return [result_text]
                
        except Exception as e:
            logger.error(f"Error suggesting synergies: {e}")
            return []
    
    def _summarize_context(self, context: Dict[str, Any]) -> str:
        """Summarize cognitive context for LLM processing."""
        summary_parts = []
        
        if "atoms" in context:
            atom_count = len(context["atoms"])
            summary_parts.append(f"- {atom_count} atoms in memory")
        
        if "patterns" in context:
            pattern_count = len(context["patterns"])
            summary_parts.append(f"- {pattern_count} discovered patterns")
        
        if "processes" in context:
            process_count = len(context["processes"])
            summary_parts.append(f"- {process_count} active cognitive processes")
        
        if "high_attention_atoms" in context:
            summary_parts.append(f"- High attention atoms: {', '.join(context['high_attention_atoms'][:5])}")
        
        return "\n".join(summary_parts) if summary_parts else "No context available"


class NeuralSymbolicBridge:
    """
    Bridges neural (LLM) and symbolic (hypergraph) representations
    for enhanced cognitive synergy.
    """
    
    def __init__(self, llm_augmenter: LLMCognitiveAugmenter):
        self.llm = llm_augmenter
        self.symbol_to_embedding_cache = {}
    
    def natural_language_to_atoms(self, text: str) -> List[Dict[str, Any]]:
        """
        Convert natural language text to cognitive atoms.
        
        Args:
            text: Natural language text
            
        Returns:
            List of atom specifications
        """
        prompt = f"""Convert this natural language text into cognitive atoms:

Text: "{text}"

Create atoms representing concepts, relationships, and properties.
Provide atoms in JSON format:
{{
    "atoms": [
        {{
            "atom_type": "Concept/Link/Property",
            "name": "atom name",
            "truth_value": 0.0-1.0,
            "metadata": {{"key": "value"}}
        }}
    ]
}}"""

        try:
            response = self.llm.client.chat.completions.create(
                model=self.llm.model,
                messages=[
                    {"role": "system", "content": "You are a knowledge representation expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=400
            )
            
            result_text = response.choices[0].message.content
            result = json.loads(result_text)
            return result.get("atoms", [])
            
        except Exception as e:
            logger.error(f"Error converting NL to atoms: {e}")
            return []
    
    def atoms_to_natural_language(self, atoms: List[Dict[str, Any]]) -> str:
        """
        Convert cognitive atoms to natural language explanation.
        
        Args:
            atoms: List of atoms
            
        Returns:
            Natural language description
        """
        atoms_summary = "\n".join([
            f"- {atom.get('atom_type', 'Unknown')}: {atom.get('name', 'unnamed')}"
            for atom in atoms[:10]  # Limit to avoid token limits
        ])
        
        prompt = f"""Convert these cognitive atoms into a natural language explanation:

Atoms:
{atoms_summary}

Provide a clear, concise explanation of what these atoms represent."""

        try:
            response = self.llm.client.chat.completions.create(
                model=self.llm.model,
                messages=[
                    {"role": "system", "content": "You are a knowledge explanation expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                max_tokens=200
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error converting atoms to NL: {e}")
            return "Unable to generate explanation"


if __name__ == "__main__":
    # Test the LLM integration
    logging.basicConfig(level=logging.INFO)
    
    print("Testing LLM Cognitive Integration...")
    print()
    
    augmenter = LLMCognitiveAugmenter()
    
    # Test hypothesis generation
    context = {
        "atoms": [{"atom_type": "Concept", "name": "Learning"}],
        "processes": [{"process_id": "reasoning", "process_type": "symbolic"}]
    }
    
    hypothesis = augmenter.generate_hypothesis(
        context,
        "How can we improve pattern recognition performance?"
    )
    
    print("Generated Hypothesis:")
    print(f"  Text: {hypothesis.hypothesis_text}")
    print(f"  Confidence: {hypothesis.confidence}")
    print(f"  Reasoning: {hypothesis.reasoning_trace}")
    print()
    
    # Test neural-symbolic bridge
    bridge = NeuralSymbolicBridge(augmenter)
    atoms = bridge.natural_language_to_atoms("The cat sat on the mat")
    
    print("Natural Language to Atoms:")
    for atom in atoms:
        print(f"  - {atom.get('atom_type')}: {atom.get('name')}")

